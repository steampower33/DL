{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMOfbdPjjOnG",
        "outputId": "51bd1d58-659e-4552-ae64-d8acbc951e12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from kaggle) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: requests in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from kaggle) (4.66.5)\n",
            "Requirement already satisfied: python-slugify in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from kaggle) (2.2.2)\n",
            "Requirement already satisfied: bleach in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from kaggle) (4.1.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from bleach->kaggle) (24.1)\n",
            "Requirement already satisfied: webencodings in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from requests->kaggle) (3.7)\n",
            "Requirement already satisfied: colorama in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from tqdm->kaggle) (0.4.6)\n",
            "Name: kaggle\n",
            "Version: 1.6.17\n",
            "Summary: Kaggle API\n",
            "Home-page: https://github.com/Kaggle/kaggle-api\n",
            "Author: Kaggle\n",
            "Author-email: support@kaggle.com\n",
            "License: Apache 2.0\n",
            "Location: c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages\n",
            "Requires: bleach, certifi, python-dateutil, python-slugify, requests, six, tqdm, urllib3\n",
            "Required-by: \n",
            "Configuration values from C:\\Users\\lee\\.kaggle\n",
            "- username: seungminl\n",
            "- path: None\n",
            "- proxy: None\n",
            "- competition: None\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install kaggle --upgrade\n",
        "!pip show kaggle\n",
        "\n",
        "!kaggle config view\n",
        "\n",
        "if not os.path.exists(\"dogs-vs-cats.zip\"):\n",
        "\t!kaggle competitions download -c dogs-vs-cats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3LvGEv2vjgb4"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "path = os.getcwd()\n",
        "data = os.path.join(path,'data')\n",
        "train_data = os.path.join(data,'train')\n",
        "test_data = os.path.join(data,'test1')\n",
        "\n",
        "# 압축파일 풀기 :하위 폴더 data에 풀기\n",
        "if not os.path.exists(\"data\"):\n",
        "\tzip_file = zipfile.ZipFile(os.path.join(path, \"dogs-vs-cats.zip\"))\n",
        "\tzip_file.extractall(path=data)\n",
        "\tzip_file.close()\n",
        "\n",
        "zip_file = zipfile.ZipFile(os.path.join(data,'train.zip'))\n",
        "zip_file.extractall(path=data)\n",
        "zip_file.close()\n",
        "\n",
        "zip_file = zipfile.ZipFile(os.path.join(data,'test1.zip'))\n",
        "zip_file.extractall(path=data)\n",
        "zip_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0Lwvs6ojgep",
        "outputId": "0a40f349-b161-45d2-f007-325cbdd5dca9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25000\n",
            "12500\n"
          ]
        }
      ],
      "source": [
        "# 데이터의 갯수\n",
        "trainFiles = os.listdir(train_data)\n",
        "testFiles = os.listdir(test_data)\n",
        "# train : 12500개 cat, 12500개 dog\n",
        "print(len(trainFiles))\n",
        "# test : 레이블이 없는 이미지 12500개\n",
        "print(len(testFiles))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_IqexPfPjpFi"
      },
      "outputs": [],
      "source": [
        "sdata = 'sdata'\n",
        "tain_sdata = os.path.join(sdata,'train')\n",
        "valid_sdata = os.path.join(sdata,'valid')\n",
        "test_sdata = os.path.join(sdata,'test')\n",
        "\n",
        "os.makedirs(tain_sdata, exist_ok=True)\n",
        "os.makedirs(valid_sdata, exist_ok=True)\n",
        "os.makedirs(test_sdata, exist_ok=True)\n",
        "\n",
        "train_sdata_dog = os.path.join(tain_sdata,'dog')\n",
        "train_sdata_cat = os.path.join(tain_sdata,'cat')\n",
        "os.makedirs(train_sdata_dog, exist_ok=True)\n",
        "os.makedirs(train_sdata_cat, exist_ok=True)\n",
        "\n",
        "valid_sdata_dog = os.path.join(valid_sdata,'dog')\n",
        "valid_sdata_cat = os.path.join(valid_sdata,'cat')\n",
        "os.makedirs(valid_sdata_dog, exist_ok=True)\n",
        "os.makedirs(valid_sdata_cat, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4R_0ZYtjpIA",
        "outputId": "02d3e84e-e6d2-4ba9-e901-5db97bac5a83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 999]\n",
            "[1000, 1499]\n",
            "[1500, 2499]\n"
          ]
        }
      ],
      "source": [
        "# 한 클래스의 이미지 갯수 * 2\n",
        "train_num = 1000\n",
        "valid_num = 500\n",
        "test_num  = 1000\n",
        "\n",
        "train_range = [0, train_num-1]\n",
        "valid_range = [train_num, train_num + valid_num -1]\n",
        "test_range  = [train_num + valid_num, train_num + valid_num + test_num-1]\n",
        "\n",
        "print(train_range)\n",
        "print(valid_range)\n",
        "print(test_range)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6q3QJ6EcjOp1",
        "outputId": "708472ce-cc7d-4259-ac18-9508a3dc830f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "데이터셋 분할 및 복사가 완료되었습니다.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# 데이터셋 디렉토리 경로 설정 (데이터셋이 저장된 경로로 수정하세요)\n",
        "dataset_dir = 'data/train'\n",
        "\n",
        "# 새로운 train, valid, test 디렉토리 생성 경로\n",
        "base_dir = 'sdata'\n",
        "\n",
        "# 클래스 목록\n",
        "classes = ['dog', 'cat']\n",
        "\n",
        "# 폴더 경로 생성\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "valid_dir = os.path.join(base_dir, 'valid')\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "\n",
        "# 폴더 생성 함수\n",
        "def create_dir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "# train, valid, test 폴더 및 각각의 클래스 폴더 생성\n",
        "for cls in classes:\n",
        "    create_dir(os.path.join(train_dir, cls))\n",
        "    create_dir(os.path.join(valid_dir, cls))\n",
        "\n",
        "# 이미지 복사 함수\n",
        "def copy_images(start_idx, end_idx, src_dir, dst_dir, label):\n",
        "    for i in range(start_idx, end_idx + 1):\n",
        "        file_name = f'{label}.{i}.jpg'\n",
        "        src_path = os.path.join(src_dir, file_name)\n",
        "        dst_path = os.path.join(dst_dir, file_name)\n",
        "        if os.path.exists(src_path): \n",
        "            shutil.copy(src_path, dst_path)\n",
        "\n",
        "# 클래스별로 train, valid, test 데이터셋 구성\n",
        "for cls in classes:\n",
        "    # train dataset 구성 (0~999)\n",
        "    copy_images(train_range[0], train_range[1], dataset_dir, os.path.join(train_dir, cls), cls)\n",
        "\n",
        "    # valid dataset 구성 (1000~1249)\n",
        "    copy_images(valid_range[0], valid_range[1], dataset_dir, os.path.join(valid_dir, cls), cls)\n",
        "\n",
        "    # test dataset 구성 (1250~1499)\n",
        "    copy_images(test_range[0], test_range[1], dataset_dir, os.path.join(test_dir), cls)\n",
        "\n",
        "print(\"데이터셋 분할 및 복사가 완료되었습니다.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iyo5lSi2j4PV",
        "outputId": "e9d288de-871f-45ad-90c8-56f9701c9487"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: timm in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (1.0.9)\n",
            "Requirement already satisfied: torch in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from timm) (2.4.1)\n",
            "Requirement already satisfied: torchvision in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from timm) (0.19.1)\n",
            "Requirement already satisfied: pyyaml in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface_hub in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from timm) (0.25.1)\n",
            "Requirement already satisfied: safetensors in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from timm) (0.4.5)\n",
            "Requirement already satisfied: filelock in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from huggingface_hub->timm) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from huggingface_hub->timm) (2024.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from huggingface_hub->timm) (24.1)\n",
            "Requirement already satisfied: requests in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from huggingface_hub->timm) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from huggingface_hub->timm) (4.11.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from torch->timm) (1.13.2)\n",
            "Requirement already satisfied: networkx in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from torch->timm) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from torch->timm) (3.1.4)\n",
            "Requirement already satisfied: numpy in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from torchvision->timm) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from torchvision->timm) (10.4.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub->timm) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from jinja2->torch->timm) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from requests->huggingface_hub->timm) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from requests->huggingface_hub->timm) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from requests->huggingface_hub->timm) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from sympy->torch->timm) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "cYOkZJ9Qx6pA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "import timm  # PyTorch Image Models (timm) 라이브러리\n",
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Z0KhLuH0M0Z",
        "outputId": "687409aa-c418-4014-febe-71d90f8dda41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-ignite in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (0.5.1)\n",
            "Requirement already satisfied: torch<3,>=1.3 in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from pytorch-ignite) (2.4.1)\n",
            "Requirement already satisfied: packaging in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from pytorch-ignite) (24.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from torch<3,>=1.3->pytorch-ignite) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from torch<3,>=1.3->pytorch-ignite) (4.11.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from torch<3,>=1.3->pytorch-ignite) (1.13.2)\n",
            "Requirement already satisfied: networkx in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from torch<3,>=1.3->pytorch-ignite) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from torch<3,>=1.3->pytorch-ignite) (3.1.4)\n",
            "Requirement already satisfied: fsspec in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from torch<3,>=1.3->pytorch-ignite) (2024.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from jinja2->torch<3,>=1.3->pytorch-ignite) (2.1.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\lee\\miniconda3\\envs\\pytorch\\lib\\site-packages (from sympy->torch<3,>=1.3->pytorch-ignite) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-ignite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "CAqPYnFHyGLn"
      },
      "outputs": [],
      "source": [
        "# Kaggle의 Dogs vs. Cats 데이터셋을 위한 CustomDataset 클래스 생성\n",
        "class DogsVsCatsDataset(Dataset):\n",
        "    def __init__(self, file_paths, labels, transform=None):\n",
        "        self.file_paths = file_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.file_paths[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "xFURQMzYyIo8"
      },
      "outputs": [],
      "source": [
        "# 파일 경로와 레이블을 생성하는 함수\n",
        "def get_file_paths_and_labels(dataset_dir):\n",
        "    file_paths = []\n",
        "    labels = []\n",
        "\n",
        "    for label, class_name in enumerate(['cat', 'dog']):\n",
        "        class_dir = os.path.join(dataset_dir, class_name)\n",
        "        for file_name in os.listdir(class_dir):\n",
        "            file_paths.append(os.path.join(class_dir, file_name))\n",
        "            labels.append(label)  # cat=0, dog=1\n",
        "\n",
        "    return file_paths, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "snXvw3YHyBHs"
      },
      "outputs": [],
      "source": [
        "# 데이터셋 디렉토리 경로 설정 (Kaggle 데이터셋 경로 설정)\n",
        "train_dir = 'sdata/train'  # train 데이터셋 경로\n",
        "valid_dir = 'sdata/valid'  # valid 데이터셋 경로\n",
        "\n",
        "# 학습 데이터셋 및 검증 데이터셋에 대한 파일 경로 및 레이블 생성\n",
        "train_file_paths, train_labels = get_file_paths_and_labels(train_dir)\n",
        "valid_file_paths, valid_labels = get_file_paths_and_labels(valid_dir)\n",
        "\n",
        "# 이미지 전처리 설정\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "valid_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# 데이터셋 준비\n",
        "train_dataset = DogsVsCatsDataset(train_file_paths, train_labels, transform=train_transform)\n",
        "valid_dataset = DogsVsCatsDataset(valid_file_paths, valid_labels, transform=valid_transform)\n",
        "\n",
        "# 데이터 로더 준비\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "iAuQbFdzjOsW"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cb9769c877c64c99b5d14bce557d0a18",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/21.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# EfficientNetV2B0 모델 불러오기 (timm 라이브러리 사용)\n",
        "model = timm.create_model('efficientnet_b0', pretrained=True)\n",
        "\n",
        "# 마지막 레이어의 입력 특징 수를 가져옵니다.\n",
        "num_features = model.classifier.in_features\n",
        "\n",
        "# 이진 분류를 위한 새로운 분류기 추가 (마지막 레이어 수정)\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Linear(num_features, 1),  # 출력 노드를 1개로 설정 (이진 분류)\n",
        "    nn.Sigmoid()  # Sigmoid 활성화 함수 추가\n",
        ")\n",
        "\n",
        "# 모델을 GPU 또는 CPU에 할당\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# 손실 함수와 최적화 알고리즘 설정\n",
        "criterion = nn.BCELoss()  # 이진 크로스 엔트로피 손실 함수\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "# 학습율을 조정\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "XsVZ1oXNQHX8"
      },
      "outputs": [],
      "source": [
        "# EarlyStopping 클래스\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): 성능 개선이 없을 때 몇 번의 에포크까지 기다릴지.\n",
        "            verbose (bool): True일 경우 개선될 때마다 메시지 출력.\n",
        "            delta (float): 성능 개선으로 간주될 최소 변화량.\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = float('inf')\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        score = -val_loss\n",
        "        # 처음에 호출됐을때는 best_score가 None이라서 초기값을 설정\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        # 지금까지의 best_score와 현재 score를 비교\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            # patience값이 모두 충족했을때, 종료조건이 만족될때\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''검증 손실이 감소하면 모델을 저장합니다.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "            torch.save(model.state_dict(), self.path)  # 모델 상태 저장\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "# EarlyStopping 인스턴스 생성 (patience=10)\n",
        "early_stopping = EarlyStopping(patience=10, verbose=True, path='efficientnet_b0_best.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "CK9IpXGwyX8U"
      },
      "outputs": [],
      "source": [
        "# 모델 학습 함수\n",
        "def train_model(model, criterion, optimizer, train_loader, valid_loader, epochs):\n",
        "    # 훈련 시작시 초기 lr을 로딩\n",
        "    pre_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "\n",
        "        # Training\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device).float()\n",
        "\n",
        "            # 모델 초기화\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward propagation\n",
        "            outputs = model(inputs).squeeze()  # (batch_size, 1) -> (batch_size,)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward propagation\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # 손실과 정확도 계산\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            preds = (outputs >= 0.5).float()  # 0.5 이상은 1, 이하 0으로 이진화\n",
        "            correct += torch.sum(preds == labels)\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_acc = correct.double() / len(train_loader.dataset)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in valid_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device).float()\n",
        "\n",
        "                outputs = model(inputs).squeeze()\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "                preds = (outputs >= 0.5).float()\n",
        "                val_correct += torch.sum(preds == labels)\n",
        "\n",
        "        val_epoch_loss = val_loss / len(valid_loader.dataset)\n",
        "        val_epoch_acc = val_correct.double() / len(valid_loader.dataset)\n",
        "\n",
        "        print(f'valid_loss: {val_epoch_loss:.4f}, valid_acc: {val_epoch_acc:.4f}')\n",
        "\n",
        "        # ReduceLROnPlateau 스케줄러를 사용하여 검증 손실에 따라 학습률을 조정\n",
        "        scheduler.step(val_epoch_loss)\n",
        "\n",
        "        # 현재 학습률 출력\n",
        "        now_lr = optimizer.param_groups[0]['lr']\n",
        "        if now_lr != pre_lr:\n",
        "            pre_lr = now_lr\n",
        "            lr_str = ', LR changed!!'\n",
        "        else:\n",
        "            lr_str = ''\n",
        "\n",
        "        print(f'learning_rate {epoch+1}: {now_lr:.8f}'+lr_str)\n",
        "\n",
        "        # EarlyStopping을 호출하여 학습 중단 여부 확인\n",
        "        early_stopping(val_epoch_loss, model)\n",
        "\n",
        "        # 학습 중단 조건을 충족하면 break\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "        print('-' * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEAwohcGyboA",
        "outputId": "bf5fa37e-75e3-4ea3-f455-d11471c088ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30, Loss: 0.2836, Accuracy: 0.9350\n",
            "valid_loss: 0.0711, valid_acc: 0.9800\n",
            "learning_rate 1: 0.00010000\n",
            "Validation loss decreased (inf --> 0.071130).  Saving model ...\n",
            "----------------------------------------------------------------------\n",
            "Epoch 2/30, Loss: 0.0648, Accuracy: 0.9820\n",
            "valid_loss: 0.0468, valid_acc: 0.9850\n",
            "learning_rate 2: 0.00010000\n",
            "Validation loss decreased (0.071130 --> 0.046776).  Saving model ...\n",
            "----------------------------------------------------------------------\n",
            "Epoch 3/30, Loss: 0.0174, Accuracy: 0.9980\n",
            "valid_loss: 0.0421, valid_acc: 0.9820\n",
            "learning_rate 3: 0.00010000\n",
            "Validation loss decreased (0.046776 --> 0.042104).  Saving model ...\n",
            "----------------------------------------------------------------------\n",
            "Epoch 4/30, Loss: 0.0116, Accuracy: 0.9985\n",
            "valid_loss: 0.0386, valid_acc: 0.9840\n",
            "learning_rate 4: 0.00010000\n",
            "Validation loss decreased (0.042104 --> 0.038649).  Saving model ...\n",
            "----------------------------------------------------------------------\n",
            "Epoch 5/30, Loss: 0.0061, Accuracy: 1.0000\n",
            "valid_loss: 0.0403, valid_acc: 0.9850\n",
            "learning_rate 5: 0.00010000\n",
            "EarlyStopping counter: 1 out of 10\n",
            "----------------------------------------------------------------------\n",
            "Epoch 6/30, Loss: 0.0060, Accuracy: 0.9985\n",
            "valid_loss: 0.0418, valid_acc: 0.9850\n",
            "learning_rate 6: 0.00010000\n",
            "EarlyStopping counter: 2 out of 10\n",
            "----------------------------------------------------------------------\n",
            "Epoch 7/30, Loss: 0.0044, Accuracy: 0.9995\n",
            "valid_loss: 0.0403, valid_acc: 0.9850\n",
            "learning_rate 7: 0.00010000\n",
            "EarlyStopping counter: 3 out of 10\n",
            "----------------------------------------------------------------------\n",
            "Epoch 8/30, Loss: 0.0035, Accuracy: 0.9995\n",
            "valid_loss: 0.0453, valid_acc: 0.9840\n",
            "learning_rate 8: 0.00005000, LR changed!!\n",
            "EarlyStopping counter: 4 out of 10\n",
            "----------------------------------------------------------------------\n",
            "Epoch 9/30, Loss: 0.0030, Accuracy: 0.9995\n",
            "valid_loss: 0.0413, valid_acc: 0.9870\n",
            "learning_rate 9: 0.00005000\n",
            "EarlyStopping counter: 5 out of 10\n",
            "----------------------------------------------------------------------\n",
            "Epoch 10/30, Loss: 0.0025, Accuracy: 1.0000\n",
            "valid_loss: 0.0465, valid_acc: 0.9840\n",
            "learning_rate 10: 0.00005000\n",
            "EarlyStopping counter: 6 out of 10\n",
            "----------------------------------------------------------------------\n",
            "Epoch 11/30, Loss: 0.0049, Accuracy: 0.9995\n",
            "valid_loss: 0.0442, valid_acc: 0.9890\n",
            "learning_rate 11: 0.00005000\n",
            "EarlyStopping counter: 7 out of 10\n",
            "----------------------------------------------------------------------\n",
            "Epoch 12/30, Loss: 0.0063, Accuracy: 0.9990\n",
            "valid_loss: 0.0462, valid_acc: 0.9850\n",
            "learning_rate 12: 0.00002500, LR changed!!\n",
            "EarlyStopping counter: 8 out of 10\n",
            "----------------------------------------------------------------------\n",
            "Epoch 13/30, Loss: 0.0026, Accuracy: 1.0000\n",
            "valid_loss: 0.0460, valid_acc: 0.9850\n",
            "learning_rate 13: 0.00002500\n",
            "EarlyStopping counter: 9 out of 10\n",
            "----------------------------------------------------------------------\n",
            "Epoch 14/30, Loss: 0.0020, Accuracy: 1.0000\n",
            "valid_loss: 0.0500, valid_acc: 0.9840\n",
            "learning_rate 14: 0.00002500\n",
            "EarlyStopping counter: 10 out of 10\n",
            "Early stopping triggered.\n"
          ]
        }
      ],
      "source": [
        "# 모델 학습\n",
        "train_model(model, criterion, optimizer, train_loader, valid_loader, epochs=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "4u5GlYXhwNl8"
      },
      "outputs": [],
      "source": [
        "modelPath = 'model'\n",
        "if not os.path.exists(modelPath):\n",
        "\tos.makedirs(modelPath)\n",
        "modelname = 'efficientnet_b0.pth'\n",
        "\n",
        "torch.save(model.state_dict(), os.path.join(modelPath, modelname))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvPa0NKnw46t",
        "outputId": "3057b261-ecd5-4939-a16a-84c6d45215d7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\lee\\AppData\\Local\\Temp\\ipykernel_53240\\1921608777.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(os.path.join(modelPath, modelname)))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 모델에 저장된 가중치 로드\n",
        "model.load_state_dict(torch.load(os.path.join(modelPath, modelname)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "5XTT_RGjbCtt"
      },
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# EfficientNetV2B0 모델 불러오기 (timm 라이브러리 사용)\n",
        "model = timm.create_model('efficientnet_b0', pretrained=True)\n",
        "\n",
        "# EfficientNet-B0의 특징 추출기(frozen)를 고정시키기\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False  # 모든 파라미터 고정\n",
        "\n",
        "# 마지막 레이어의 입력 특징 수를 가져옵니다.\n",
        "num_features = model.classifier.in_features\n",
        "\n",
        "# 이진 분류를 위한 새로운 분류기 추가 (마지막 레이어 수정)\n",
        "model.classifier = nn.Sequential(\n",
        "    torch.nn.Linear(num_features, 256),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Dropout(0.5),\n",
        "    torch.nn.Linear(256, 1),\n",
        "    torch.nn.Sigmoid()\n",
        ")\n",
        "\n",
        "# 분류기 부분만 학습되도록 파라미터 업데이트 설정\n",
        "for param in model.classifier.parameters():\n",
        "    param.requires_grad = True  # 분류기 부분만 학습 가능하게 설정\n",
        "\n",
        "# 모델을 GPU 또는 CPU에 할당\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# 손실 함수와 최적화 알고리즘 설정\n",
        "criterion = nn.BCELoss()  # 이진 크로스 엔트로피 손실 함수\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
        "early_stopping = EarlyStopping(patience=10, verbose=True, path='efficientnet_b0_best.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HM9skLcvcCsd",
        "outputId": "d729fdd7-4725-46e2-c85f-528a914e30d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5, Loss: 0.5376, Accuracy: 0.8370\n",
            "valid_loss: 0.3771, valid_acc: 0.9350\n",
            "learning_rate 1: 0.00010000\n",
            "Validation loss decreased (inf --> 0.377127).  Saving model ...\n",
            "----------------------------------------------------------------------\n",
            "Epoch 2/5, Loss: 0.3044, Accuracy: 0.9225\n",
            "valid_loss: 0.2351, valid_acc: 0.9510\n",
            "learning_rate 2: 0.00010000\n",
            "Validation loss decreased (0.377127 --> 0.235079).  Saving model ...\n",
            "----------------------------------------------------------------------\n",
            "Epoch 3/5, Loss: 0.2106, Accuracy: 0.9370\n",
            "valid_loss: 0.1828, valid_acc: 0.9560\n",
            "learning_rate 3: 0.00010000\n",
            "Validation loss decreased (0.235079 --> 0.182784).  Saving model ...\n",
            "----------------------------------------------------------------------\n",
            "Epoch 4/5, Loss: 0.1793, Accuracy: 0.9480\n",
            "valid_loss: 0.1545, valid_acc: 0.9630\n",
            "learning_rate 4: 0.00010000\n",
            "Validation loss decreased (0.182784 --> 0.154523).  Saving model ...\n",
            "----------------------------------------------------------------------\n",
            "Epoch 5/5, Loss: 0.1395, Accuracy: 0.9620\n",
            "valid_loss: 0.1327, valid_acc: 0.9670\n",
            "learning_rate 5: 0.00010000\n",
            "Validation loss decreased (0.154523 --> 0.132748).  Saving model ...\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "train_model(model, criterion, optimizer, train_loader, valid_loader, epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I64u7Y33d4sn",
        "outputId": "9e793917-e295-4310-8fe6-940d5de23c59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "EfficientNet(\n",
              "  (conv_stem): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "  (bn1): BatchNormAct2d(\n",
              "    32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "    (drop): Identity()\n",
              "    (act): SiLU(inplace=True)\n",
              "  )\n",
              "  (blocks): Sequential(\n",
              "    (0): Sequential(\n",
              "      (0): DepthwiseSeparableConv(\n",
              "        (conv_dw): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pw): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "    )\n",
              "    (1): Sequential(\n",
              "      (0): InvertedResidual(\n",
              "        (conv_pw): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (1): InvertedResidual(\n",
              "        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "    )\n",
              "    (2): Sequential(\n",
              "      (0): InvertedResidual(\n",
              "        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (1): InvertedResidual(\n",
              "        (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "    )\n",
              "    (3): Sequential(\n",
              "      (0): InvertedResidual(\n",
              "        (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (1): InvertedResidual(\n",
              "        (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (2): InvertedResidual(\n",
              "        (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "    )\n",
              "    (4): Sequential(\n",
              "      (0): InvertedResidual(\n",
              "        (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (1): InvertedResidual(\n",
              "        (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (2): InvertedResidual(\n",
              "        (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "    )\n",
              "    (5): Sequential(\n",
              "      (0): InvertedResidual(\n",
              "        (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (1): InvertedResidual(\n",
              "        (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (2): InvertedResidual(\n",
              "        (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "      (3): InvertedResidual(\n",
              "        (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "    )\n",
              "    (6): Sequential(\n",
              "      (0): InvertedResidual(\n",
              "        (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNormAct2d(\n",
              "          1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (conv_dw): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
              "        (bn2): BatchNormAct2d(\n",
              "          1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): SiLU(inplace=True)\n",
              "        )\n",
              "        (aa): Identity()\n",
              "        (se): SqueezeExcite(\n",
              "          (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): SiLU(inplace=True)\n",
              "          (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (gate): Sigmoid()\n",
              "        )\n",
              "        (conv_pwl): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNormAct2d(\n",
              "          320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "          (drop): Identity()\n",
              "          (act): Identity()\n",
              "        )\n",
              "        (drop_path): Identity()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (conv_head): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "  (bn2): BatchNormAct2d(\n",
              "    1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "    (drop): Identity()\n",
              "    (act): SiLU(inplace=True)\n",
              "  )\n",
              "  (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=1280, out_features=256, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=256, out_features=1, bias=True)\n",
              "    (4): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "cnven_kYeWmh"
      },
      "outputs": [],
      "source": [
        "def display_parameters(model):\n",
        "    # 학습 가능한 파라미터의 개수 출력\n",
        "    num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Number of trainable parameters: {num_trainable_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "nAnzvYJCef5w"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of trainable parameters: 0\n"
          ]
        }
      ],
      "source": [
        "for param in model.classifier.parameters():\n",
        "    param.requires_grad = False  # 분류기 부분만 학습 가능하게 설정\n",
        "\n",
        "display_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "crgUkfp6coHg"
      },
      "outputs": [],
      "source": [
        "# 마지막 두 블록을 가져와서 requires_grad를 True로 설정\n",
        "for param in model.blocks[-2:].parameters():\n",
        "    param.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMVGhaMed0ud",
        "outputId": "4ee2e62f-7702-490c-d768-416e11b3a534"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of trainable parameters: 2743580\n"
          ]
        }
      ],
      "source": [
        "# 특정 레이어 이름을 기준으로 동결 해제\n",
        "for name, param in model.named_parameters():\n",
        "    if \"blocks.5\" in name or \"blocks.6\" in name:  # 예를 들어, 마지막 두 블록\n",
        "        param.requires_grad = True\n",
        "\n",
        "display_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRu1HTL2e_pz",
        "outputId": "44eb3be3-dda4-4e2e-818d-4157a84805f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "conv_stem.weight\n",
            "bn1.weight\n",
            "bn1.bias\n",
            "blocks.0.0.conv_dw.weight\n",
            "blocks.0.0.bn1.weight\n",
            "blocks.0.0.bn1.bias\n",
            "blocks.0.0.se.conv_reduce.weight\n",
            "blocks.0.0.se.conv_reduce.bias\n",
            "blocks.0.0.se.conv_expand.weight\n",
            "blocks.0.0.se.conv_expand.bias\n",
            "blocks.0.0.conv_pw.weight\n",
            "blocks.0.0.bn2.weight\n",
            "blocks.0.0.bn2.bias\n",
            "blocks.1.0.conv_pw.weight\n",
            "blocks.1.0.bn1.weight\n",
            "blocks.1.0.bn1.bias\n",
            "blocks.1.0.conv_dw.weight\n",
            "blocks.1.0.bn2.weight\n",
            "blocks.1.0.bn2.bias\n",
            "blocks.1.0.se.conv_reduce.weight\n",
            "blocks.1.0.se.conv_reduce.bias\n",
            "blocks.1.0.se.conv_expand.weight\n",
            "blocks.1.0.se.conv_expand.bias\n",
            "blocks.1.0.conv_pwl.weight\n",
            "blocks.1.0.bn3.weight\n",
            "blocks.1.0.bn3.bias\n",
            "blocks.1.1.conv_pw.weight\n",
            "blocks.1.1.bn1.weight\n",
            "blocks.1.1.bn1.bias\n",
            "blocks.1.1.conv_dw.weight\n",
            "blocks.1.1.bn2.weight\n",
            "blocks.1.1.bn2.bias\n",
            "blocks.1.1.se.conv_reduce.weight\n",
            "blocks.1.1.se.conv_reduce.bias\n",
            "blocks.1.1.se.conv_expand.weight\n",
            "blocks.1.1.se.conv_expand.bias\n",
            "blocks.1.1.conv_pwl.weight\n",
            "blocks.1.1.bn3.weight\n",
            "blocks.1.1.bn3.bias\n",
            "blocks.2.0.conv_pw.weight\n",
            "blocks.2.0.bn1.weight\n",
            "blocks.2.0.bn1.bias\n",
            "blocks.2.0.conv_dw.weight\n",
            "blocks.2.0.bn2.weight\n",
            "blocks.2.0.bn2.bias\n",
            "blocks.2.0.se.conv_reduce.weight\n",
            "blocks.2.0.se.conv_reduce.bias\n",
            "blocks.2.0.se.conv_expand.weight\n",
            "blocks.2.0.se.conv_expand.bias\n",
            "blocks.2.0.conv_pwl.weight\n",
            "blocks.2.0.bn3.weight\n",
            "blocks.2.0.bn3.bias\n",
            "blocks.2.1.conv_pw.weight\n",
            "blocks.2.1.bn1.weight\n",
            "blocks.2.1.bn1.bias\n",
            "blocks.2.1.conv_dw.weight\n",
            "blocks.2.1.bn2.weight\n",
            "blocks.2.1.bn2.bias\n",
            "blocks.2.1.se.conv_reduce.weight\n",
            "blocks.2.1.se.conv_reduce.bias\n",
            "blocks.2.1.se.conv_expand.weight\n",
            "blocks.2.1.se.conv_expand.bias\n",
            "blocks.2.1.conv_pwl.weight\n",
            "blocks.2.1.bn3.weight\n",
            "blocks.2.1.bn3.bias\n",
            "blocks.3.0.conv_pw.weight\n",
            "blocks.3.0.bn1.weight\n",
            "blocks.3.0.bn1.bias\n",
            "blocks.3.0.conv_dw.weight\n",
            "blocks.3.0.bn2.weight\n",
            "blocks.3.0.bn2.bias\n",
            "blocks.3.0.se.conv_reduce.weight\n",
            "blocks.3.0.se.conv_reduce.bias\n",
            "blocks.3.0.se.conv_expand.weight\n",
            "blocks.3.0.se.conv_expand.bias\n",
            "blocks.3.0.conv_pwl.weight\n",
            "blocks.3.0.bn3.weight\n",
            "blocks.3.0.bn3.bias\n",
            "blocks.3.1.conv_pw.weight\n",
            "blocks.3.1.bn1.weight\n",
            "blocks.3.1.bn1.bias\n",
            "blocks.3.1.conv_dw.weight\n",
            "blocks.3.1.bn2.weight\n",
            "blocks.3.1.bn2.bias\n",
            "blocks.3.1.se.conv_reduce.weight\n",
            "blocks.3.1.se.conv_reduce.bias\n",
            "blocks.3.1.se.conv_expand.weight\n",
            "blocks.3.1.se.conv_expand.bias\n",
            "blocks.3.1.conv_pwl.weight\n",
            "blocks.3.1.bn3.weight\n",
            "blocks.3.1.bn3.bias\n",
            "blocks.3.2.conv_pw.weight\n",
            "blocks.3.2.bn1.weight\n",
            "blocks.3.2.bn1.bias\n",
            "blocks.3.2.conv_dw.weight\n",
            "blocks.3.2.bn2.weight\n",
            "blocks.3.2.bn2.bias\n",
            "blocks.3.2.se.conv_reduce.weight\n",
            "blocks.3.2.se.conv_reduce.bias\n",
            "blocks.3.2.se.conv_expand.weight\n",
            "blocks.3.2.se.conv_expand.bias\n",
            "blocks.3.2.conv_pwl.weight\n",
            "blocks.3.2.bn3.weight\n",
            "blocks.3.2.bn3.bias\n",
            "blocks.4.0.conv_pw.weight\n",
            "blocks.4.0.bn1.weight\n",
            "blocks.4.0.bn1.bias\n",
            "blocks.4.0.conv_dw.weight\n",
            "blocks.4.0.bn2.weight\n",
            "blocks.4.0.bn2.bias\n",
            "blocks.4.0.se.conv_reduce.weight\n",
            "blocks.4.0.se.conv_reduce.bias\n",
            "blocks.4.0.se.conv_expand.weight\n",
            "blocks.4.0.se.conv_expand.bias\n",
            "blocks.4.0.conv_pwl.weight\n",
            "blocks.4.0.bn3.weight\n",
            "blocks.4.0.bn3.bias\n",
            "blocks.4.1.conv_pw.weight\n",
            "blocks.4.1.bn1.weight\n",
            "blocks.4.1.bn1.bias\n",
            "blocks.4.1.conv_dw.weight\n",
            "blocks.4.1.bn2.weight\n",
            "blocks.4.1.bn2.bias\n",
            "blocks.4.1.se.conv_reduce.weight\n",
            "blocks.4.1.se.conv_reduce.bias\n",
            "blocks.4.1.se.conv_expand.weight\n",
            "blocks.4.1.se.conv_expand.bias\n",
            "blocks.4.1.conv_pwl.weight\n",
            "blocks.4.1.bn3.weight\n",
            "blocks.4.1.bn3.bias\n",
            "blocks.4.2.conv_pw.weight\n",
            "blocks.4.2.bn1.weight\n",
            "blocks.4.2.bn1.bias\n",
            "blocks.4.2.conv_dw.weight\n",
            "blocks.4.2.bn2.weight\n",
            "blocks.4.2.bn2.bias\n",
            "blocks.4.2.se.conv_reduce.weight\n",
            "blocks.4.2.se.conv_reduce.bias\n",
            "blocks.4.2.se.conv_expand.weight\n",
            "blocks.4.2.se.conv_expand.bias\n",
            "blocks.4.2.conv_pwl.weight\n",
            "blocks.4.2.bn3.weight\n",
            "blocks.4.2.bn3.bias\n",
            "blocks.5.0.conv_pw.weight\n",
            "blocks.5.0.bn1.weight\n",
            "blocks.5.0.bn1.bias\n",
            "blocks.5.0.conv_dw.weight\n",
            "blocks.5.0.bn2.weight\n",
            "blocks.5.0.bn2.bias\n",
            "blocks.5.0.se.conv_reduce.weight\n",
            "blocks.5.0.se.conv_reduce.bias\n",
            "blocks.5.0.se.conv_expand.weight\n",
            "blocks.5.0.se.conv_expand.bias\n",
            "blocks.5.0.conv_pwl.weight\n",
            "blocks.5.0.bn3.weight\n",
            "blocks.5.0.bn3.bias\n",
            "blocks.5.1.conv_pw.weight\n",
            "blocks.5.1.bn1.weight\n",
            "blocks.5.1.bn1.bias\n",
            "blocks.5.1.conv_dw.weight\n",
            "blocks.5.1.bn2.weight\n",
            "blocks.5.1.bn2.bias\n",
            "blocks.5.1.se.conv_reduce.weight\n",
            "blocks.5.1.se.conv_reduce.bias\n",
            "blocks.5.1.se.conv_expand.weight\n",
            "blocks.5.1.se.conv_expand.bias\n",
            "blocks.5.1.conv_pwl.weight\n",
            "blocks.5.1.bn3.weight\n",
            "blocks.5.1.bn3.bias\n",
            "blocks.5.2.conv_pw.weight\n",
            "blocks.5.2.bn1.weight\n",
            "blocks.5.2.bn1.bias\n",
            "blocks.5.2.conv_dw.weight\n",
            "blocks.5.2.bn2.weight\n",
            "blocks.5.2.bn2.bias\n",
            "blocks.5.2.se.conv_reduce.weight\n",
            "blocks.5.2.se.conv_reduce.bias\n",
            "blocks.5.2.se.conv_expand.weight\n",
            "blocks.5.2.se.conv_expand.bias\n",
            "blocks.5.2.conv_pwl.weight\n",
            "blocks.5.2.bn3.weight\n",
            "blocks.5.2.bn3.bias\n",
            "blocks.5.3.conv_pw.weight\n",
            "blocks.5.3.bn1.weight\n",
            "blocks.5.3.bn1.bias\n",
            "blocks.5.3.conv_dw.weight\n",
            "blocks.5.3.bn2.weight\n",
            "blocks.5.3.bn2.bias\n",
            "blocks.5.3.se.conv_reduce.weight\n",
            "blocks.5.3.se.conv_reduce.bias\n",
            "blocks.5.3.se.conv_expand.weight\n",
            "blocks.5.3.se.conv_expand.bias\n",
            "blocks.5.3.conv_pwl.weight\n",
            "blocks.5.3.bn3.weight\n",
            "blocks.5.3.bn3.bias\n",
            "blocks.6.0.conv_pw.weight\n",
            "blocks.6.0.bn1.weight\n",
            "blocks.6.0.bn1.bias\n",
            "blocks.6.0.conv_dw.weight\n",
            "blocks.6.0.bn2.weight\n",
            "blocks.6.0.bn2.bias\n",
            "blocks.6.0.se.conv_reduce.weight\n",
            "blocks.6.0.se.conv_reduce.bias\n",
            "blocks.6.0.se.conv_expand.weight\n",
            "blocks.6.0.se.conv_expand.bias\n",
            "blocks.6.0.conv_pwl.weight\n",
            "blocks.6.0.bn3.weight\n",
            "blocks.6.0.bn3.bias\n",
            "conv_head.weight\n",
            "bn2.weight\n",
            "bn2.bias\n",
            "classifier.0.weight\n",
            "classifier.0.bias\n",
            "classifier.3.weight\n",
            "classifier.3.bias\n"
          ]
        }
      ],
      "source": [
        "for name, param in model.named_parameters():\n",
        "    print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmGi_2qnfwqB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
